# 获取句子向量方法概述（An Overview of Sentence Embedding Methods）
*[本文主要介绍如何获取句子向量--在计算句子相似度时非常有用，大部分内容都是[这篇文章](http://mlexplained.com/2017/12/28/an-overview-of-sentence-embedding-methods/)的翻译，另外加进了一些自己的总结以及中文网络上的讨论内容。]
*
在基于神经网络的NLP研究中，词嵌入/向量是一种非常强大、有效的方法。到现在，word2vec和Glove已经是获取词向量的标准方法了（虽然也还有其他几种方法）。

与此形成对比的是，获取句子向量的方法仍然还不太明确（more elusive）。虽然可以通过监督学习训练词向量来得到句子向量，但通常情况下我们希望通过非监督的方式获取句子向量。例如，我们可能需要进行释义识别（paraphrase identification）或者建立一个高效获取相似句子的系统，但是并没有可以用来进行监督训练的数据。

在本文中，我将介绍几种非监督方式获取句子向量的重要方法。这绝对算不上全面，还有更多其他的方法；如果我遗漏了哪个重要方法，请在评论中指出，我好改进本文。
<br>

## 1.Topic Models
虽然经常处在神经网络类方法的阴影下，对句子向量来说，主题模型仍然是一种非常强大且可解释的框架。

## 2.Paragraph Vectors (doc2vec)

## 3.Skip-Thought Vectors

## 4.FastSent

## 5.加权词向量

*[以下内容由译者增加]
关于该论文，

注意：根据其他人的讨论，基于词向量的计算句子向量比较适合短文本，在长句子/文本上表现不好。
[结束]
*
## 6.其他方法

*[以下内容由译者增加]*
## 7.基于词向量
之前在bat做过这个，和大家分享一下。
先说一个还是从词的角度出发考虑的，最后的效果非常好，就是怎么样从词的向量得到句子的向量，首先选出一个词库，比如说10万个词，然后用w2v跑出所有词的向量，然后对于每一个句子，构造一个10万维的向量，向量的每一维是该维对应的词和该句子中每一个词的相似度的最大值。这种方法实际上是bag of words的一个扩展，比如说对于 我喜欢用苹果手机 这么一句话，对应的向量，会在三星，诺基亚，小米，电脑等词上也会有比较高的得分。这种做法对于bag of words的稀疏性问题效果非常好。
(作者：鲁灵犀 链接：https://www.zhihu.com/question/29978268/answer/55338644)

## 8. RNN/LTSM
用lstm做句子encoding吧，不需要word2vec这种，根据句子上下文做训练；
不行的话用denoising Autoencoder，这种效果最好哦；
也可以试试看seq2seq的最终向量，输入输出都是句子本身,完全无监督；
实在不行只能用那个啥variational Autoencoder了，用kl divergence计算句子的距离；
(作者：斐斐晏 链接：https://www.zhihu.com/question/29978268/answer/180167488)

## 9.一些补充讨论
###1.如何定义句子的相似性？这个可能决定我们需要什么样的句子向量。
原始的word2vec只对words之间的关系进行建模。题主要做句子相似度计算，首先需要定义清楚一个问题：“句子相似”是什么，“句子不相似”是什么？，别看这两个定义naive，仔细考虑一下会发现，这两个定义不同，则句子相似度的建模方法差别也会很大。
如果，我们采用最简单的定义：“句子相似”=“句子中的词语相似”，则用word2vec得到的词向量相加，然后归一化一下，是可以较好得代表句子的。然而，如果希望model到词语顺序和语法结构恐怕就不能用这种方法。此外，对句子进行建模的paper是有的，只是大量都不是为了得到句子的embedding，而是为了处理一些传统NLP任务，比如分词(使用HMM, CTR)等等。
BTW.  这篇 Distributed Representations of Sentences and Documents 参考意义不大，**只要还停留在word2vec的框架内，就无法model到语法结构对句子相似度的影响**。
(作者：li Eta 链接：https://www.zhihu.com/question/29978268/answer/54389427)

*[结束]*
